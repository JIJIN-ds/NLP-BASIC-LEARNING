{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05a66d76",
   "metadata": {},
   "source": [
    "Genism = 'Generate Similar'\n",
    "- popular  open source NLP library used for unsupervised topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3d5282",
   "metadata": {},
   "source": [
    "Important terms and meanings\n",
    "\n",
    "document = some text\n",
    "\n",
    "corpus = a collection of document\n",
    "\n",
    "vector= a mathematically convenient representation of a documentn \n",
    "\n",
    "model = an algorithm for transforming vextor from one to another representationm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e5c7694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "#create a set of frequent words\n",
    "stoplist=set('for a of the and to in is an'.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c114fb18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a', 'an', 'and', 'for', 'in', 'is', 'of', 'the', 'to'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoplist   #similar to word tokenize in form of set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "613ded24",
   "metadata": {},
   "outputs": [],
   "source": [
    "textcorpus= [\"Human interface for lab\",\"Robots for future Human\",\"Data Science is emerging field\",\"Graph minors the survey future of Data Science\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f4e62c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lowercase each document, split it by white space and filter out stopwords\n",
    "\n",
    "text=[[word for word in document.lower().split() if word not in stoplist]for document in textcorpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b48a8f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'lab'],\n",
       " ['robots', 'future', 'human'],\n",
       " ['data', 'science', 'emerging', 'field'],\n",
       " ['graph', 'minors', 'survey', 'future', 'data', 'science']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ad1399a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting collection\n",
      "  Using cached collection-0.1.6-py3-none-any.whl\n",
      "Installing collected packages: collection\n",
      "Successfully installed collection-0.1.6\n"
     ]
    }
   ],
   "source": [
    "#!pip install collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7aabe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count word frequencies\n",
    "#!pip install collection\n",
    "from collections import defaultdict\n",
    "frequency=defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1afe7c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "017d5ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating each and every word frequency\n",
    "for texts in text:\n",
    "    for token in texts:\n",
    "        frequency[token]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "339dcaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only keep words that appear more than once\n",
    "\n",
    "processed_corpus=[[token for token in texts if frequency[token]>1]for texts in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82f1206c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human'],\n",
      " ['future', 'human'],\n",
      " ['data', 'science'],\n",
      " ['future', 'data', 'science']]\n"
     ]
    }
   ],
   "source": [
    "#we got data having frequencies higher than 1\n",
    "pprint.pprint(processed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "669ccc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is an simple example of pprint and defaultdict from collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6046e91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(4 unique tokens: ['human', 'future', 'data', 'science'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jijin\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#Creating dictionaries which help during topic handling (using gensim)\n",
    "\n",
    "#to get data separately from above\n",
    "\n",
    "from gensim import corpora\n",
    "dictionary=corpora.Dictionary(processed_corpus)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4888840f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': 2, 'future': 1, 'human': 0, 'science': 3}\n"
     ]
    }
   ],
   "source": [
    "#Vector Representation\n",
    "\n",
    "pprint.pprint(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6ea135",
   "metadata": {},
   "source": [
    "Model : An algorithm for transferring vectors from one representation to Another. \n",
    "    \n",
    "Always make a pratice of testing small text when trying something new\n",
    "\n",
    "In each tuple below 1st occurance is ID and 2nd occurance is count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a50a010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test #box of words\n",
    "\n",
    "new_doc=\"Human computer interaction\"\n",
    "new_vec= dictionary.doc2bow(new_doc.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8f3a689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(new_vec) #In each tuple below 1st occurance is ID and 2nd occurance is count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f117a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus=[dictionary.doc2bow(text) for text in processed_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cdac1c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1)], [(0, 1), (1, 1)], [(2, 1), (3, 1)], [(1, 1), (2, 1), (3, 1)]]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d197c351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5773502691896258), (2, 0.5773502691896258), (3, 0.5773502691896258)]\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "\n",
    "from gensim import models\n",
    "\n",
    "#train the model\n",
    "\n",
    "tfidf=models.TfidfModel(bow_corpus)\n",
    "\n",
    "#Transform the \"system minors data\" string\n",
    "\n",
    "word= \"human System minors DATA science\".lower().split()\n",
    "print(tfidf[dictionary.doc2bow(word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f524ecaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is how we know the weightage of words in a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1036c2e",
   "metadata": {},
   "source": [
    "# Open text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aaacef9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'amongst': 0, 'applied': 1, 'being': 2, 'bunch': 3, 'chance': 4, 'fibonacci': 5, 'get': 6, 'given': 7, 'got': 8, 'hackerrank': 9, 'holiday': 10, 'homes': 11, 'hr': 12, 'indian': 13, 'interviewed': 14, 'met': 15, 'of': 16, 'on': 17, 'platform': 18, 'python': 19, 'questions': 20, 'sequence': 21, 'startup': 22, 'the': 23, 'them': 24, 'there': 25, 'to': 26, 'toughest': 27, 'was': 28, 'with': 29, 'about': 30, 'all': 31, 'and': 32, 'another': 33, 'answer': 34, 'but': 35, 'completed': 36, 'could': 37, 'couldnâ': 38, 'didnâ': 39, 'felt': 40, 'field': 41, 'from': 42, 'guy': 43, 'he': 44, 'in': 45, 'interested': 46, 'interview': 47, 'interviewerâ': 48, 'it': 49, 'job': 50, 'learning': 51, 'less': 52, 'made': 53, 'me': 54, 'ml': 55, 'my': 56, 'negativity': 57, 'professional': 58, 'side': 59, 'some': 60, 'somewhat': 61, 'sound': 62, 'starting': 63, 'statistics': 64, 'successfully': 65, 'talking': 66, 'then': 67, 'this': 68, 'those': 69, 'took': 70, 'well': 71, 'went': 72, 'which': 73, 'who': 74, 'amused': 75, 'answered': 76, 'asked': 77, 'at': 78, 'behavioural': 79, 'by': 80, 'culture': 81, 'fact': 82, 'fidelity': 83, 'fit': 84, 'for': 85, 'founder': 86, 'full': 87, 'good': 88, 'had': 89, 'highly': 90, 'him': 91, 'his': 92, 'honest': 93, 'honestly': 94, 'if': 95, 'judge': 96, 'just': 97, 'know': 98, 'leave': 99, 'mattered': 100, 'much': 101, 'other': 102, 'person': 103, 'polite': 104, 'pretty': 105, 'question': 106, 'secure': 107, 'that': 108, 'their': 109, 'time': 110, 'times': 111, 'too': 112, 'compensation': 113, 'day': 114, 'decision': 115, 'details': 116, 'discussed': 117, 'final': 118, 'found': 119, 'go': 120, 'home': 121, 'interviewing': 122, 'iâ': 123, 'later': 124, 'let': 125, 'll': 126, 'next': 127, 'offer': 128, 'people': 129, 'selected': 130, 'telling': 131, 'thatâ': 132, 'thing': 133, 'told': 134, 'why': 135, 'work': 136, 'age': 137, 'amazing': 138, 'an': 139, 'became': 140, 'connections': 141, 'data': 142, 'development': 143, 'domain': 144, 'early': 145, 'experience': 146, 'focus': 147, 'gain': 148, 'gave': 149, 'head': 150, 'idea': 151, 'initial': 152, 'interviews': 153, 'managing': 154, 'more': 155, 'moved': 156, 'not': 157, 'one': 158, 'position': 159, 'related': 160, 'tech': 161, 'technology': 162, 'very': 163, 'wanted': 164, 'we': 165}\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import os\n",
    "dict_STF = corpora.Dictionary(simple_preprocess(line) for line in open(r\"sample.txt\"))\n",
    "print(dict_STF.token2id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
