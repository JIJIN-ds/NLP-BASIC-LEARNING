{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ad8929f",
   "metadata": {},
   "source": [
    "# Basics of NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e80e413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTALLING LIBRARIES\n",
    "##!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5235e0",
   "metadata": {},
   "source": [
    "Tokenization :\n",
    "    Tokenization is a process of breaking down a given paragraph of text into a list of sentence or words. when paragraph is broken down into list of sentences, it is called sentence tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884af040",
   "metadata": {},
   "source": [
    "# Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61cda480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Iam', 'learning', 'natural', 'language', 'processing']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "#IMPORTING REQUIRED FILE\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "#Define your text or import from other source\n",
    "text='Iam learning natural language processing'\n",
    "#tokenizing\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6650ba",
   "metadata": {},
   "source": [
    "# Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3d906d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['our company growth is 25%.', 'Good job Mr.Balaji']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text=\"our company growth is 25%. Good job Mr.Balaji\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c8bdab",
   "metadata": {},
   "source": [
    "# Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9226bf8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is',\n",
       " 'fun',\n",
       " 'and',\n",
       " 'can',\n",
       " 'deal',\n",
       " 'with',\n",
       " 'texts',\n",
       " 'and',\n",
       " 'sounds',\n",
       " 'but',\n",
       " 'can',\n",
       " 't',\n",
       " 'deal',\n",
       " 'with',\n",
       " 'images',\n",
       " 'we',\n",
       " 'have',\n",
       " 'session',\n",
       " 'at',\n",
       " 'we',\n",
       " 'can',\n",
       " 'earn',\n",
       " 'lot',\n",
       " 'of']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize \n",
    "#Sample text\n",
    "text=\"NLP is fun and can deal with texts and sounds, but can't deal with images. we have session at 11 AM! we can earn lot of $\"\n",
    "#Print word by word that contains all small case and starts from all small case and starts from small a to z. \n",
    "regexp_tokenize(text,\"[a-z]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f131e0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is',\n",
       " 'fun',\n",
       " 'and',\n",
       " 'can',\n",
       " 'deal',\n",
       " 'with',\n",
       " 'texts',\n",
       " 'and',\n",
       " 'sounds',\n",
       " 'but',\n",
       " \"can't\",\n",
       " 'deal',\n",
       " 'with',\n",
       " 'images',\n",
       " 'we',\n",
       " 'have',\n",
       " 'session',\n",
       " 'at',\n",
       " 'we',\n",
       " 'can',\n",
       " 'earn',\n",
       " 'lot',\n",
       " 'of']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extra quote gets you word like can't,don't\n",
    "regexp_tokenize(text,\"[a-z']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "645e4e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLP', 'AM']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Print word by word that contains all caps from A to Z\n",
    "regexp_tokenize(text,\"[A-Z]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11623701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"NLP is fun and can deal with texts and sounds, but can't deal with images. we have session at 11 AM! we can earn lot of $\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to get everything in one line\n",
    "regexp_tokenize(text,\"[\\a-z]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d508a46e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLP ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ', ',\n",
       " ' ',\n",
       " \"'\",\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " '. ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' 11 AM! ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' $']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Anything start with caret symbol ^ is taken as not equal to\n",
    "\n",
    "regexp_tokenize(text,\"[^a-z]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddbf0b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['11']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#only numbers\n",
    "regexp_tokenize(text,\"[0-9]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afcdb014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"NLP is fun and can deal with texts and sounds, but can't deal with images. we have session at \",\n",
       " ' AM! we can earn lot of $']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#without number\n",
    "regexp_tokenize(text,\"[^0-9]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7838083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dollar symbol from text\n",
    "regexp_tokenize(text,\"[$]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5c3d22",
   "metadata": {},
   "source": [
    "# STOP WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa900e97",
   "metadata": {},
   "source": [
    "stopwords are commonly occuring words which doesn't have any meaning or impact for processing. It is removed in pre processing (a,an,the,at etc,.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e015f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is how we import stop words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78db55b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you are getting error on this you can also do nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ed5b592",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jijin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97bf0032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#there will be 179 words default present in stopwords\n",
    "stopwords=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6e92373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b49fda1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eab0c01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also do it simply reducing steps\n",
    "stopset=set(nltk.corpus.stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51419c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57cfc018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "12040c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding custom stopwords\n",
    "stopset.update(('new','old'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b792b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b654469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "#Similar to stop words we can also ignore punctuations in our sentence\n",
    "\n",
    "# import string for getting punctuations\n",
    "\n",
    "import string\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "15c3c9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original length ==>  143\n",
      "Length of cleaned text ==> 15\n",
      "\n",
      " ['India', 'Hindi', 'Bharat', 'officially', 'republic', 'india', 'country', 'south', 'asia', 'It', 'seventh', 'largest', 'country', 'size', 'population']\n"
     ]
    }
   ],
   "source": [
    "# Excercise : Remove stopwords and punctuations from the text \n",
    "\n",
    "text = \"India (Hindi : Bharat),officially the republic of india, is a country in south asia. It is the seventh largest country by size and population ;\"\n",
    "\n",
    "# creating an empty list top load the cleaned data\n",
    "cleaned_text=[]\n",
    "\n",
    "punct=string.punctuation\n",
    "stopwords=set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "for word in nltk.word_tokenize(text):\n",
    "    if word not in punct:\n",
    "        if word not in stopwords:\n",
    "            cleaned_text.append(word)\n",
    "            \n",
    "print('Original length ==> ',len(text))\n",
    "print('Length of cleaned text ==>',len(cleaned_text))\n",
    "print('\\n',cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2b1b2d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "india (hindi : bharat),officially the republic of india, is a country in south asia. it is the seventh largest country by size and population ;\n"
     ]
    }
   ],
   "source": [
    "#It is taken as it contains caps I \n",
    "#convert into lowercase\n",
    "print(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eadf5ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text= text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "80870568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original length ==>  143\n",
      "Length of cleaned text ==> 14\n",
      "\n",
      " ['india', 'hindi', 'bharat', 'officially', 'republic', 'india', 'country', 'south', 'asia', 'seventh', 'largest', 'country', 'size', 'population']\n"
     ]
    }
   ],
   "source": [
    "# creating an empty list top load the cleaned data\n",
    "cleaned_text=[]\n",
    "\n",
    "punct=string.punctuation\n",
    "stopwords=set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "for word in nltk.word_tokenize(text):\n",
    "    if word not in punct:\n",
    "        if word not in stopwords:\n",
    "            cleaned_text.append(word)\n",
    "            \n",
    "print('Original length ==> ',len(text))\n",
    "print('Length of cleaned text ==>',len(cleaned_text))\n",
    "print('\\n',cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "781d4725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDIA (HINDI : BHARAT),OFFICIALLY THE REPUBLIC OF INDIA, IS A COUNTRY IN SOUTH ASIA. IT IS THE SEVENTH LARGEST COUNTRY BY SIZE AND POPULATION ;\n"
     ]
    }
   ],
   "source": [
    "print(text.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a0df54",
   "metadata": {},
   "source": [
    "# STEMMING --- Lemmatization ---- Wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7de4d1",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "940054f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8ee809bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lancaster= LancasterStemmer()\n",
    "porter=PorterStemmer()\n",
    "snowball=SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cccac3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer\n",
      "hobbi\n",
      "hobbi\n",
      "comput\n",
      "comput\n",
      "****************************************************************************************************\n",
      "lancaster Stemmer\n",
      "hobby\n",
      "hobby\n",
      "comput\n",
      "comput\n",
      "****************************************************************************************************\n",
      "Snowball Stemmer\n",
      "hobbi\n",
      "hobbi\n",
      "comput\n",
      "comput\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "#stemming just stems the word without looking into grammatical mistake. Let us see one of the example\n",
    "#lancaster algo is faster than porter but is more complex. Porter stemmer is the oldest algo present and was the most popular to use\n",
    "#Snowball stemmer, also known as porter 2,is the updated version of porter stemmer and is currently the most popular stemming algorithm\n",
    "#Snowball stemmer available for multiple language\n",
    "\n",
    "print('Porter Stemmer')\n",
    "print(porter.stem(\"hobby\"))\n",
    "print(porter.stem(\"hobbies\"))\n",
    "print(porter.stem(\"computer\"))\n",
    "print(porter.stem(\"computation\"))\n",
    "print(\"****************************************************************************************************\")\n",
    "\n",
    "print('lancaster Stemmer')\n",
    "print(lancaster.stem(\"hobby\"))\n",
    "print(lancaster.stem(\"hobbies\"))\n",
    "print(lancaster.stem(\"computer\"))\n",
    "print(lancaster.stem(\"computation\"))\n",
    "print(\"****************************************************************************************************\")\n",
    "\n",
    "print('Snowball Stemmer')\n",
    "print(snowball.stem(\"hobby\"))\n",
    "print(snowball.stem(\"hobbies\"))\n",
    "print(snowball.stem(\"computer\"))\n",
    "print(snowball.stem(\"computation\"))\n",
    "print(\"****************************************************************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1b17c2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "run\n",
      "ran\n"
     ]
    }
   ],
   "source": [
    "#Another example\n",
    "\n",
    "print(porter.stem(\"running\"))\n",
    "print(porter.stem(\"runs\"))\n",
    "print(porter.stem(\"ran\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7738eba6",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5a59dc",
   "metadata": {},
   "source": [
    "lemmitization is same as stemming but ensures grammatical mistake. we us word net lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1fad380e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jijin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "151bad82",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "721c0fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "run\n",
      "ran\n"
     ]
    }
   ],
   "source": [
    "print(lemma.lemmatize(\"running\"))\n",
    "print(lemma.lemmatize(\"runs\"))\n",
    "print(lemma.lemmatize(\"ran\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d73944f",
   "metadata": {},
   "source": [
    "We can see how it takes care of meaning of a word. But it is a time consuming process. Hence use it only when there is a necessity for the meaning and context of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5d589b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "# We can pass POS tags for the words in a sentence\n",
    "\n",
    "print(lemma.lemmatize(\"running\",pos='v'))\n",
    "print(lemma.lemmatize(\"runs\",pos='v'))\n",
    "print(lemma.lemmatize(\"ran\",pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043ee03f",
   "metadata": {},
   "source": [
    "we can see how it changes the part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "468f0830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming\n",
      "The stemming for Bring is bring\n",
      "The stemming for king is king\n",
      "The stemming for going is go\n",
      "The stemming for anything is anyth\n",
      "The stemming for sing is sing\n",
      "The stemming for ring is ring\n",
      "The stemming for nothing is noth\n",
      "The stemming for thing is thing\n",
      "***************************************************************\n",
      "Lemmatizing\n",
      "The lemmatizing of Bring is Bring\n",
      "The lemmatizing of king is king\n",
      "The lemmatizing of going is going\n",
      "The lemmatizing of anything is anything\n",
      "The lemmatizing of sing is sing\n",
      "The lemmatizing of ring is ring\n",
      "The lemmatizing of nothing is nothing\n",
      "The lemmatizing of thing is thing\n"
     ]
    }
   ],
   "source": [
    "#Excercise using both stemming and lemma\n",
    "\n",
    "text=\"Bring king going anything sing ring nothing thing\"\n",
    "\n",
    "#Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "\n",
    "print(\"Stemming\")\n",
    "for word in nltk.tokenize.word_tokenize(text):\n",
    "    print(\"The stemming for {} is {}\".format(word,ps.stem(word)))\n",
    "print(\"***************************************************************\")   \n",
    "\n",
    "#Lammetization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma=WordNetLemmatizer()\n",
    "\n",
    "print(\"Lemmatizing\")\n",
    "for word in nltk.tokenize.word_tokenize(text):\n",
    "    print(\"The lemmatizing of {} is {}\".format(word,lemma.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ac6d1d",
   "metadata": {},
   "source": [
    "# WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220ac1e5",
   "metadata": {},
   "source": [
    "It is used to find the meaning of words synonym and antonym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9654223d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonyms = >  {'participating', 'active_agent', 'active', 'dynamic', 'alive', 'active_voice', 'fighting', 'combat-ready'}\n",
      "Antonyms = >  {'stative', 'inactive', 'passive', 'extinct', 'passive_voice', 'quiet', 'dormant'}\n"
     ]
    }
   ],
   "source": [
    "#Let's find synonyms and antonyms using code\n",
    "\n",
    "from nltk.corpus import wordnet \n",
    "\n",
    "synonym=[]\n",
    "antonym=[]\n",
    "\n",
    "#example\n",
    "for syn in wordnet.synsets(\"active\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonym.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonym.append(l.antonyms()[0].name())\n",
    "\n",
    "\n",
    "print('Synonyms = > ',set(synonym))\n",
    "print('Antonyms = > ',set(antonym))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "41b49746",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn = wordnet.synsets(\"active\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c10e8574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('active_agent.n.01'),\n",
       " Synset('active_voice.n.01'),\n",
       " Synset('active.n.03'),\n",
       " Synset('active.a.01'),\n",
       " Synset('active.s.02'),\n",
       " Synset('active.a.03'),\n",
       " Synset('active.s.04'),\n",
       " Synset('active.a.05'),\n",
       " Synset('active.a.06'),\n",
       " Synset('active.a.07'),\n",
       " Synset('active.s.08'),\n",
       " Synset('active.a.09'),\n",
       " Synset('active.a.10'),\n",
       " Synset('active.a.11'),\n",
       " Synset('active.a.12'),\n",
       " Synset('active.a.13'),\n",
       " Synset('active.a.14')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2127813d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('active_agent.n.01.active_agent'), Lemma('active_agent.n.01.active')]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn[0].lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2c203e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'active_agent'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn[0].lemmas()[0].name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "19f6a287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'active'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn[0].lemmas()[1].name()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96903867",
   "metadata": {},
   "source": [
    "# Word Embedding , 1_count Vectorization,BOW, n_gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ec7390",
   "metadata": {},
   "source": [
    "Count Vectorization : Vectorizes the given words on the basis of frequency of words\n",
    "    \n",
    "Types\n",
    "\n",
    "Bag of words model (BOW)\n",
    "\n",
    "n_gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92a6df6",
   "metadata": {},
   "source": [
    "BOW : no grammers required, just keeps the account of words in a particular document\n",
    "\n",
    "mainly used for feature selection , weights are given to most occuring word. Since stop words will be present, we can't take most occuring word as important . To resolve this issue , \"Tf_idf\" introduced\n",
    "\n",
    "\n",
    "n_gram : n_gram is sued in such cases to keep the context of the given text intact. N_gram is the sequence of n words from a given text/document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7251ec85",
   "metadata": {},
   "source": [
    "n1=unigram\n",
    "n2= bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfb4fed",
   "metadata": {},
   "source": [
    "text1= \"I went to have a cup of coffee\"\n",
    "\n",
    "unigram  =['I','went','to','have','a','cup','of','coffee']\n",
    "\n",
    "bigram   =['I went','to have','a cup','of coffee']\n",
    "\n",
    "trigram  =['I went to','have a cup','of coffee']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494b0efd",
   "metadata": {},
   "source": [
    "Skip gram : skip gram are type of n gram where the words are not necessarily in the same order as are in the given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "65cabef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us see some examples of Count vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37073675",
   "metadata": {},
   "source": [
    "#### Bag of words (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c8c755d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words  : ['an', 'bag', 'example', 'is', 'of', 'this', 'words']\n"
     ]
    }
   ],
   "source": [
    "# example of single document #without stop words \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "string =[\" This is an example of bag of words\"]  #Single document\n",
    "#converting'\n",
    "vect1=CountVectorizer()\n",
    "vect1.fit_transform(string)\n",
    "print(\"bag of words  :\",vect1.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "63ed87c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 5, 'is': 3, 'an': 0, 'example': 2, 'of': 4, 'bag': 1, 'words': 6}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect1.vocabulary_  #gives indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "aeb96baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit and transform and predict if the word is present or not (This is widely used for document (or) subject classification)\n",
    "\n",
    "c_vect=CountVectorizer()\n",
    "c_vect.fit(string)  #Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "93844ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text present at second string similar to string 1  [[0 1 0 2 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "string2=[\"Lets understand what is bag of words is\"]\n",
    "\n",
    "c_new_vect=c_vect.transform(string2)   #Testing \n",
    "print(\"Text present at second string similar to string 1 \",c_new_vect.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abcc387",
   "metadata": {},
   "source": [
    "['an'=0, 'bag'=1, 'example'=0, 'is'=2, 'of'=1, 'this'=0, 'words'=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e64aa06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Everything is fine, but you can see we can't determine more important words due to stopwords. \n",
    "\n",
    "stpwords=stopwords.words('english')\n",
    "vect2=CountVectorizer(stop_words=stpwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a719c79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words :  ['bag', 'lets', 'understand', 'words']\n",
      "Index number : {'lets': 1, 'understand': 2, 'bag': 0, 'words': 3}\n"
     ]
    }
   ],
   "source": [
    "vect2.fit_transform(string2)\n",
    "print(\"bag of words : \",vect2.get_feature_names())\n",
    "print(\"Index number :\",vect2.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2bd3ed",
   "metadata": {},
   "source": [
    "Now there are no stopwords seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cdb87b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating userdefined function to use the block of code again and again \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7fd4485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BOW(message,text,cv): #cv=CountVECTORIZER\n",
    "    cv.fit(message)\n",
    "    filtext=cv.transform(text)\n",
    "    return pd.DataFrame(filtext.toarray(),columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b1273a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer(stop_words=stpwords)  #stpwords=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "071fb4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "message=['We are slowly slowly making progress in Natural language Processing','we will get there','But practice is the only mantra']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "05a349d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=[\"Natural language Processing is very interesting and will help us practice better making good progress with text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3f54ccec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>get</th>\n",
       "      <th>language</th>\n",
       "      <th>making</th>\n",
       "      <th>mantra</th>\n",
       "      <th>natural</th>\n",
       "      <th>practice</th>\n",
       "      <th>processing</th>\n",
       "      <th>progress</th>\n",
       "      <th>slowly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   get  language  making  mantra  natural  practice  processing  progress  \\\n",
       "0    0         1       1       0        1         1           1         1   \n",
       "\n",
       "   slowly  \n",
       "0       0  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOW(message,text,cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e6192988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>get</th>\n",
       "      <th>language</th>\n",
       "      <th>making</th>\n",
       "      <th>mantra</th>\n",
       "      <th>natural</th>\n",
       "      <th>practice</th>\n",
       "      <th>processing</th>\n",
       "      <th>progress</th>\n",
       "      <th>slowly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   get  language  making  mantra  natural  practice  processing  progress  \\\n",
       "0    0         1       1       0        1         0           1         1   \n",
       "1    1         0       0       0        0         0           0         0   \n",
       "2    0         0       0       1        0         1           0         0   \n",
       "\n",
       "   slowly  \n",
       "0       2  \n",
       "1       0  \n",
       "2       0  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOW(message,message,cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edf625b",
   "metadata": {},
   "source": [
    "# n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f900d8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram: ['an', 'example', 'gram', 'is', 'of', 'this']\n",
      "[[1 1 1 1 1 1]]\n",
      "***********************************\n",
      "2 gram: ['an example', 'example of', 'is an', 'of gram', 'this is']\n",
      "[[1 1 1 1 1]]\n",
      "************************************\n",
      "3 gram ['an example', 'an example of', 'example of', 'example of gram', 'is an', 'is an example', 'of gram', 'this is', 'this is an']\n",
      "[[1 1 1 1 1 1 1 1 1]]\n",
      "************************************\n",
      "4 gram ['an example of gram', 'is an example of', 'this is an example']\n",
      "[[1 1 1]]\n",
      "************************************\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "string=[\"This is an example of gram \"]\n",
    "\n",
    "vect1=CountVectorizer(ngram_range=(1,1))   #similar to bag of words\n",
    "v1=vect1.fit_transform(string)\n",
    "\n",
    "vect2=CountVectorizer(ngram_range=(2,2))\n",
    "v2=vect2.fit_transform(string)\n",
    "\n",
    "vect3=CountVectorizer(ngram_range=(2,3))\n",
    "v3=vect3.fit_transform(string)\n",
    "\n",
    "vect4=CountVectorizer(ngram_range=(4,4))\n",
    "v4=vect4.fit_transform(string)\n",
    "\n",
    "print(\"1-gram:\",vect1.get_feature_names())   #similar to bag of words\n",
    "print(v1.toarray())\n",
    "print(\"***********************************\")\n",
    "print(\"2 gram:\",vect2.get_feature_names())\n",
    "print(v2.toarray())\n",
    "print(\"************************************\")\n",
    "print(\"3 gram\",vect3.get_feature_names())\n",
    "print(v3.toarray())\n",
    "print(\"************************************\")\n",
    "print(\"4 gram\",vect4.get_feature_names())\n",
    "print(v4.toarray())\n",
    "print(\"************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1645e35e",
   "metadata": {},
   "source": [
    "# Word Embedding 2 - Tf-Idf Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd2d908",
   "metadata": {},
   "source": [
    "term frequency inverse document frequency\n",
    "\n",
    "tf idf score = tf *idf\n",
    "\n",
    "tf term frequency is defined as no of times the words occur in document divided by total no of words in the document\n",
    "\n",
    "idf is defined as log of total no of document divided by the words appearing in no of document (log has base 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e1548a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>an</th>\n",
       "      <th>be</th>\n",
       "      <th>can</th>\n",
       "      <th>confusing</th>\n",
       "      <th>example</th>\n",
       "      <th>how</th>\n",
       "      <th>idf</th>\n",
       "      <th>is</th>\n",
       "      <th>it</th>\n",
       "      <th>see</th>\n",
       "      <th>this</th>\n",
       "      <th>we</th>\n",
       "      <th>will</th>\n",
       "      <th>works</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.408248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    an   be  can  confusing  example       how  idf   is        it       see  \\\n",
       "0  0.5  0.0  0.0        0.0      0.5  0.000000  0.0  0.5  0.000000  0.000000   \n",
       "1  0.0  0.0  0.0        0.0      0.0  0.408248  0.0  0.0  0.408248  0.408248   \n",
       "2  0.0  0.5  0.5        0.5      0.0  0.000000  0.5  0.0  0.000000  0.000000   \n",
       "\n",
       "   this        we      will     works  \n",
       "0   0.5  0.000000  0.000000  0.000000  \n",
       "1   0.0  0.408248  0.408248  0.408248  \n",
       "2   0.0  0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#programming\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "tfid=TfidfVectorizer(smooth_idf=False)\n",
    "doc=[\"This is an example\",\"We will see how it works\",\"IDF can be confusing\"]\n",
    "doc_vector=tfid.fit_transform(doc)\n",
    "df=pd.DataFrame(doc_vector.todense(),columns=tfid.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6d3153a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cases</th>\n",
       "      <th>covid</th>\n",
       "      <th>dropping</th>\n",
       "      <th>nothing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.430165</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.90275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.670092</td>\n",
       "      <td>0.319302</td>\n",
       "      <td>0.670092</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cases     covid  dropping  nothing\n",
       "0  0.000000  1.000000  0.000000  0.00000\n",
       "1  0.000000  0.430165  0.000000  0.90275\n",
       "2  0.670092  0.319302  0.670092  0.00000"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using function userdefined\n",
    "\n",
    "def text_matrix(message,tf):\n",
    "    term_doc=tf.fit_transform(message)\n",
    "    return pd.DataFrame(term_doc.toarray(),columns=tf.get_feature_names())\n",
    "\n",
    "feb_message=[\"what is covid covid\",\"covid is nothing\",\"covid cases are dropping\"]\n",
    "tf=TfidfVectorizer(smooth_idf=False,stop_words=stpwords) #without dictionary words\n",
    "text_matrix(feb_message,tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "68a6053c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names \n",
      " ['by car', 'by jack', 'car was', 'cleaned by', 'jack was', 'was cleaned']\n",
      "Array \n",
      " [[0 1 1 1 0 1]\n",
      " [1 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "#Countvectorizer,TF-IDF, n grams program\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "arr=[\"Car was cleaned by Jack\",\"Jack was cleaned by car\"]\n",
    "#if you want to take into account as 3 frequencies\n",
    "cv=CountVectorizer(ngram_range=(2,2))  #n gram specifies your range\n",
    "x=cv.fit_transform(arr)\n",
    "#Testing the ngram generation\n",
    "print(\"Feature Names \\n\",cv.get_feature_names())\n",
    "print(\"Array \\n\",x.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f90f5990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names \n",
      " ['car cleaned', 'cleaned car', 'cleaned jack', 'jack cleaned']\n",
      "Array \n",
      " [[0.70710678 0.         0.70710678 0.        ]\n",
      " [0.         0.70710678 0.         0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "#and now testing TFIDF VECTORIZER:\n",
    "#you can still specify ngrams here   #without stopwords\n",
    "\n",
    "tf=TfidfVectorizer(ngram_range=(2,2),smooth_idf=False,stop_words=stpwords)\n",
    "x=tf.fit_transform(arr)\n",
    "#testing idf value and n gram generation\n",
    "print(\"Feature names \\n\",tf.get_feature_names())\n",
    "print(\"Array \\n\",x.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "368ccca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names \n",
      " ['car cleaned', 'cleaned car', 'cleaned jack', 'jack cleaned']\n",
      "Array \n",
      " [[1.69314718 0.         1.69314718 0.        ]\n",
      " [0.         1.69314718 0.         1.69314718]]\n"
     ]
    }
   ],
   "source": [
    "#Testing TFIDF WITHOUT NORMALISATION\n",
    "tf=TfidfVectorizer(ngram_range=(2,2),smooth_idf=False,stop_words=stpwords,norm=None)\n",
    "x=tf.fit_transform(arr)\n",
    "#testing idf value and n gram generation\n",
    "print(\"Feature names \\n\",tf.get_feature_names())\n",
    "print(\"Array \\n\",x.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5e7540",
   "metadata": {},
   "source": [
    "# WORD EMBEDDING - 3WORD2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c99667c",
   "metadata": {},
   "source": [
    "Most advanced word embedding technique than count vectorizer and Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "629df3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.0.1-cp38-cp38-win_amd64.whl (23.9 MB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\jijin\\anaconda3\\lib\\site-packages (from gensim) (1.6.2)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\jijin\\anaconda3\\lib\\site-packages (from gensim) (1.20.1)\n",
      "Collecting Cython==0.29.21\n",
      "  Downloading Cython-0.29.21-cp38-cp38-win_amd64.whl (1.7 MB)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-5.1.0-py3-none-any.whl (57 kB)\n",
      "Installing collected packages: smart-open, Cython, gensim\n",
      "  Attempting uninstall: Cython\n",
      "    Found existing installation: Cython 0.29.23\n",
      "    Uninstalling Cython-0.29.23:\n",
      "      Successfully uninstalled Cython-0.29.23\n",
      "Successfully installed Cython-0.29.21 gensim-4.0.1 smart-open-5.1.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "32c8efca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-Levenshtein\n",
      "  Downloading python-Levenshtein-0.12.2.tar.gz (50 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jijin\\anaconda3\\lib\\site-packages (from python-Levenshtein) (52.0.0.post20210125)\n",
      "Building wheels for collected packages: python-Levenshtein\n",
      "  Building wheel for python-Levenshtein (setup.py): started\n",
      "  Building wheel for python-Levenshtein (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for python-Levenshtein\n",
      "Failed to build python-Levenshtein\n",
      "Installing collected packages: python-Levenshtein\n",
      "    Running setup.py install for python-Levenshtein: started\n",
      "    Running setup.py install for python-Levenshtein: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\jijin\\anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\jijin\\\\AppData\\\\Local\\\\Temp\\\\pip-install-080plyxa\\\\python-levenshtein_e848920cf8364d2b8489ce2d2677dcb8\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\jijin\\\\AppData\\\\Local\\\\Temp\\\\pip-install-080plyxa\\\\python-levenshtein_e848920cf8364d2b8489ce2d2677dcb8\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\jijin\\AppData\\Local\\Temp\\pip-wheel-opr43wnz'\n",
      "       cwd: C:\\Users\\jijin\\AppData\\Local\\Temp\\pip-install-080plyxa\\python-levenshtein_e848920cf8364d2b8489ce2d2677dcb8\\\n",
      "  Complete output (27 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-3.8\n",
      "  creating build\\lib.win-amd64-3.8\\Levenshtein\n",
      "  copying Levenshtein\\StringMatcher.py -> build\\lib.win-amd64-3.8\\Levenshtein\n",
      "  copying Levenshtein\\__init__.py -> build\\lib.win-amd64-3.8\\Levenshtein\n",
      "  running egg_info\n",
      "  writing python_Levenshtein.egg-info\\PKG-INFO\n",
      "  writing dependency_links to python_Levenshtein.egg-info\\dependency_links.txt\n",
      "  writing entry points to python_Levenshtein.egg-info\\entry_points.txt\n",
      "  writing namespace_packages to python_Levenshtein.egg-info\\namespace_packages.txt\n",
      "  writing requirements to python_Levenshtein.egg-info\\requires.txt\n",
      "  writing top-level names to python_Levenshtein.egg-info\\top_level.txt\n",
      "  reading manifest file 'python_Levenshtein.egg-info\\SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  warning: no previously-included files matching '*pyc' found anywhere in distribution\n",
      "  warning: no previously-included files matching '*so' found anywhere in distribution\n",
      "  warning: no previously-included files matching '.project' found anywhere in distribution\n",
      "  warning: no previously-included files matching '.pydevproject' found anywhere in distribution\n",
      "  writing manifest file 'python_Levenshtein.egg-info\\SOURCES.txt'\n",
      "  copying Levenshtein\\_levenshtein.c -> build\\lib.win-amd64-3.8\\Levenshtein\n",
      "  copying Levenshtein\\_levenshtein.h -> build\\lib.win-amd64-3.8\\Levenshtein\n",
      "  running build_ext\n",
      "  building 'Levenshtein._levenshtein' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for python-Levenshtein\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\jijin\\anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\jijin\\\\AppData\\\\Local\\\\Temp\\\\pip-install-080plyxa\\\\python-levenshtein_e848920cf8364d2b8489ce2d2677dcb8\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\jijin\\\\AppData\\\\Local\\\\Temp\\\\pip-install-080plyxa\\\\python-levenshtein_e848920cf8364d2b8489ce2d2677dcb8\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\jijin\\AppData\\Local\\Temp\\pip-record-oy9gluco\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\jijin\\anaconda3\\Include\\python-Levenshtein'\n",
      "         cwd: C:\\Users\\jijin\\AppData\\Local\\Temp\\pip-install-080plyxa\\python-levenshtein_e848920cf8364d2b8489ce2d2677dcb8\\\n",
      "    Complete output (27 lines):\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build\\lib.win-amd64-3.8\n",
      "    creating build\\lib.win-amd64-3.8\\Levenshtein\n",
      "    copying Levenshtein\\StringMatcher.py -> build\\lib.win-amd64-3.8\\Levenshtein\n",
      "    copying Levenshtein\\__init__.py -> build\\lib.win-amd64-3.8\\Levenshtein\n",
      "    running egg_info\n",
      "    writing python_Levenshtein.egg-info\\PKG-INFO\n",
      "    writing dependency_links to python_Levenshtein.egg-info\\dependency_links.txt\n",
      "    writing entry points to python_Levenshtein.egg-info\\entry_points.txt\n",
      "    writing namespace_packages to python_Levenshtein.egg-info\\namespace_packages.txt\n",
      "    writing requirements to python_Levenshtein.egg-info\\requires.txt\n",
      "    writing top-level names to python_Levenshtein.egg-info\\top_level.txt\n",
      "    reading manifest file 'python_Levenshtein.egg-info\\SOURCES.txt'\n",
      "    reading manifest template 'MANIFEST.in'\n",
      "    warning: no previously-included files matching '*pyc' found anywhere in distribution\n",
      "    warning: no previously-included files matching '*so' found anywhere in distribution\n",
      "    warning: no previously-included files matching '.project' found anywhere in distribution\n",
      "    warning: no previously-included files matching '.pydevproject' found anywhere in distribution\n",
      "    writing manifest file 'python_Levenshtein.egg-info\\SOURCES.txt'\n",
      "    copying Levenshtein\\_levenshtein.c -> build\\lib.win-amd64-3.8\\Levenshtein\n",
      "    copying Levenshtein\\_levenshtein.h -> build\\lib.win-amd64-3.8\\Levenshtein\n",
      "    running build_ext\n",
      "    building 'Levenshtein._levenshtein' extension\n",
      "    error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'C:\\Users\\jijin\\anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\jijin\\\\AppData\\\\Local\\\\Temp\\\\pip-install-080plyxa\\\\python-levenshtein_e848920cf8364d2b8489ce2d2677dcb8\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\jijin\\\\AppData\\\\Local\\\\Temp\\\\pip-install-080plyxa\\\\python-levenshtein_e848920cf8364d2b8489ce2d2677dcb8\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\jijin\\AppData\\Local\\Temp\\pip-record-oy9gluco\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\jijin\\anaconda3\\Include\\python-Levenshtein' Check the logs for full command output.\n"
     ]
    }
   ],
   "source": [
    "#!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "fccd654d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2Vec\n",
    "\n",
    "import gensim\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "1bdfd0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph=\"Coronavirus disease (COVID-19) is an infectious disease caused by a newly discovered coronavirus.Most people infected with the COVID-19 virus will experience mild to moderate respiratory illness and recover without requiring special treatment.  Older people, and those with underlying medical problems like cardiovascular disease, diabetes, chronic respiratory disease, and cancer are more likely to develop serious illness.The best way to prevent and slow down transmission is to be well informed about the COVID-19 virus, the disease it causes and how it spreads. Protect yourself and others from infection by washing your hands or using an alcohol based rub frequently and not touching your face. The COVID-19 virus spreads primarily through droplets of saliva or discharge from the nose when an infected person coughs or sneezes, so it is important that you also practice respiratory etiquette (for example, by coughing into a flexed elbow).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "f3184bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepocessing the data (for more details on regular expression visit https://regexr.com/)\n",
    "\n",
    "paragraph=paragraph.translate(str.maketrans('','',string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "56c7a31e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coronavirus disease COVID19 is an infectious disease caused by a newly discovered coronavirusMost people infected with the COVID19 virus will experience mild to moderate respiratory illness and recover without requiring special treatment  Older people and those with underlying medical problems like cardiovascular disease diabetes chronic respiratory disease and cancer are more likely to develop serious illnessThe best way to prevent and slow down transmission is to be well informed about the COVID19 virus the disease it causes and how it spreads Protect yourself and others from infection by washing your hands or using an alcohol based rub frequently and not touching your face The COVID19 virus spreads primarily through droplets of saliva or discharge from the nose when an infected person coughs or sneezes so it is important that you also practice respiratory etiquette for example by coughing into a flexed elbow'"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "976583c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=re.sub(r'\\[[0-9]*\\]',' ',paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "00994f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coronavirus disease COVID19 is an infectious disease caused by a newly discovered coronavirusMost people infected with the COVID19 virus will experience mild to moderate respiratory illness and recover without requiring special treatment  Older people and those with underlying medical problems like cardiovascular disease diabetes chronic respiratory disease and cancer are more likely to develop serious illnessThe best way to prevent and slow down transmission is to be well informed about the COVID19 virus the disease it causes and how it spreads Protect yourself and others from infection by washing your hands or using an alcohol based rub frequently and not touching your face The COVID19 virus spreads primarily through droplets of saliva or discharge from the nose when an infected person coughs or sneezes so it is important that you also practice respiratory etiquette for example by coughing into a flexed elbow'"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "0132e241",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=re.sub(r'\\s+',' ',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "e10c9a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coronavirus disease COVID19 is an infectious disease caused by a newly discovered coronavirusMost people infected with the COVID19 virus will experience mild to moderate respiratory illness and recover without requiring special treatment Older people and those with underlying medical problems like cardiovascular disease diabetes chronic respiratory disease and cancer are more likely to develop serious illnessThe best way to prevent and slow down transmission is to be well informed about the COVID19 virus the disease it causes and how it spreads Protect yourself and others from infection by washing your hands or using an alcohol based rub frequently and not touching your face The COVID19 virus spreads primarily through droplets of saliva or discharge from the nose when an infected person coughs or sneezes so it is important that you also practice respiratory etiquette for example by coughing into a flexed elbow'"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "751b4eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=re.sub(r'\\d+',' ',text) # to remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "c2667cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coronavirus disease COVID  is an infectious disease caused by a newly discovered coronavirusMost people infected with the COVID  virus will experience mild to moderate respiratory illness and recover without requiring special treatment Older people and those with underlying medical problems like cardiovascular disease diabetes chronic respiratory disease and cancer are more likely to develop serious illnessThe best way to prevent and slow down transmission is to be well informed about the COVID  virus the disease it causes and how it spreads Protect yourself and others from infection by washing your hands or using an alcohol based rub frequently and not touching your face The COVID  virus spreads primarily through droplets of saliva or discharge from the nose when an infected person coughs or sneezes so it is important that you also practice respiratory etiquette for example by coughing into a flexed elbow'"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "47977308",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "8c8b3d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coronavirus disease covid  is an infectious disease caused by a newly discovered coronavirusmost people infected with the covid  virus will experience mild to moderate respiratory illness and recover without requiring special treatment older people and those with underlying medical problems like cardiovascular disease diabetes chronic respiratory disease and cancer are more likely to develop serious illnessthe best way to prevent and slow down transmission is to be well informed about the covid  virus the disease it causes and how it spreads protect yourself and others from infection by washing your hands or using an alcohol based rub frequently and not touching your face the covid  virus spreads primarily through droplets of saliva or discharge from the nose when an infected person coughs or sneezes so it is important that you also practice respiratory etiquette for example by coughing into a flexed elbow'"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "c76784c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=re.sub(r'\\s+',' ',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "f4c90dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coronavirus disease covid is an infectious disease caused by a newly discovered coronavirusmost people infected with the covid virus will experience mild to moderate respiratory illness and recover without requiring special treatment older people and those with underlying medical problems like cardiovascular disease diabetes chronic respiratory disease and cancer are more likely to develop serious illnessthe best way to prevent and slow down transmission is to be well informed about the covid virus the disease it causes and how it spreads protect yourself and others from infection by washing your hands or using an alcohol based rub frequently and not touching your face the covid virus spreads primarily through droplets of saliva or discharge from the nose when an infected person coughs or sneezes so it is important that you also practice respiratory etiquette for example by coughing into a flexed elbow'"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "533a7ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['coronavirus disease covid is an infectious disease caused by a newly discovered coronavirusmost people infected with the covid virus will experience mild to moderate respiratory illness and recover without requiring special treatment older people and those with underlying medical problems like cardiovascular disease diabetes chronic respiratory disease and cancer are more likely to develop serious illnessthe best way to prevent and slow down transmission is to be well informed about the covid virus the disease it causes and how it spreads protect yourself and others from infection by washing your hands or using an alcohol based rub frequently and not touching your face the covid virus spreads primarily through droplets of saliva or discharge from the nose when an infected person coughs or sneezes so it is important that you also practice respiratory etiquette for example by coughing into a flexed elbow']"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preparing the dataset\n",
    "\n",
    "sentences=nltk.sent_tokenize(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "356699e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['coronavirus', 'disease', 'covid', 'is', 'an', 'infectious', 'disease', 'caused', 'by', 'a', 'newly', 'discovered', 'coronavirusmost', 'people', 'infected', 'with', 'the', 'covid', 'virus', 'will', 'experience', 'mild', 'to', 'moderate', 'respiratory', 'illness', 'and', 'recover', 'without', 'requiring', 'special', 'treatment', 'older', 'people', 'and', 'those', 'with', 'underlying', 'medical', 'problems', 'like', 'cardiovascular', 'disease', 'diabetes', 'chronic', 'respiratory', 'disease', 'and', 'cancer', 'are', 'more', 'likely', 'to', 'develop', 'serious', 'illnessthe', 'best', 'way', 'to', 'prevent', 'and', 'slow', 'down', 'transmission', 'is', 'to', 'be', 'well', 'informed', 'about', 'the', 'covid', 'virus', 'the', 'disease', 'it', 'causes', 'and', 'how', 'it', 'spreads', 'protect', 'yourself', 'and', 'others', 'from', 'infection', 'by', 'washing', 'your', 'hands', 'or', 'using', 'an', 'alcohol', 'based', 'rub', 'frequently', 'and', 'not', 'touching', 'your', 'face', 'the', 'covid', 'virus', 'spreads', 'primarily', 'through', 'droplets', 'of', 'saliva', 'or', 'discharge', 'from', 'the', 'nose', 'when', 'an', 'infected', 'person', 'coughs', 'or', 'sneezes', 'so', 'it', 'is', 'important', 'that', 'you', 'also', 'practice', 'respiratory', 'etiquette', 'for', 'example', 'by', 'coughing', 'into', 'a', 'flexed', 'elbow']]\n"
     ]
    }
   ],
   "source": [
    "sent_word=[nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "print(sent_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "8a0013ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punc=string.punctuation\n",
    "punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "e4f101c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sent_word)):\n",
    "    sent_word[i]=[word for word in sent_word[i] if word not in stopwords.words('english') if word not in punc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "3b95c6e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['coronavirus',\n",
       "  'disease',\n",
       "  'covid',\n",
       "  'infectious',\n",
       "  'disease',\n",
       "  'caused',\n",
       "  'newly',\n",
       "  'discovered',\n",
       "  'coronavirusmost',\n",
       "  'people',\n",
       "  'infected',\n",
       "  'covid',\n",
       "  'virus',\n",
       "  'experience',\n",
       "  'mild',\n",
       "  'moderate',\n",
       "  'respiratory',\n",
       "  'illness',\n",
       "  'recover',\n",
       "  'without',\n",
       "  'requiring',\n",
       "  'special',\n",
       "  'treatment',\n",
       "  'older',\n",
       "  'people',\n",
       "  'underlying',\n",
       "  'medical',\n",
       "  'problems',\n",
       "  'like',\n",
       "  'cardiovascular',\n",
       "  'disease',\n",
       "  'diabetes',\n",
       "  'chronic',\n",
       "  'respiratory',\n",
       "  'disease',\n",
       "  'cancer',\n",
       "  'likely',\n",
       "  'develop',\n",
       "  'serious',\n",
       "  'illnessthe',\n",
       "  'best',\n",
       "  'way',\n",
       "  'prevent',\n",
       "  'slow',\n",
       "  'transmission',\n",
       "  'well',\n",
       "  'informed',\n",
       "  'covid',\n",
       "  'virus',\n",
       "  'disease',\n",
       "  'causes',\n",
       "  'spreads',\n",
       "  'protect',\n",
       "  'others',\n",
       "  'infection',\n",
       "  'washing',\n",
       "  'hands',\n",
       "  'using',\n",
       "  'alcohol',\n",
       "  'based',\n",
       "  'rub',\n",
       "  'frequently',\n",
       "  'touching',\n",
       "  'face',\n",
       "  'covid',\n",
       "  'virus',\n",
       "  'spreads',\n",
       "  'primarily',\n",
       "  'droplets',\n",
       "  'saliva',\n",
       "  'discharge',\n",
       "  'nose',\n",
       "  'infected',\n",
       "  'person',\n",
       "  'coughs',\n",
       "  'sneezes',\n",
       "  'important',\n",
       "  'also',\n",
       "  'practice',\n",
       "  'respiratory',\n",
       "  'etiquette',\n",
       "  'example',\n",
       "  'coughing',\n",
       "  'flexed',\n",
       "  'elbow']]"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "a0cba7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training The Word2Vec Model\n",
    "model=Word2Vec(sent_word,min_count=1)\n",
    "words=model.wv.key_to_index\n",
    "words1=model.wv.get_vecattr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "2d06bf87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'disease': 0,\n",
       " 'covid': 1,\n",
       " 'respiratory': 2,\n",
       " 'virus': 3,\n",
       " 'infected': 4,\n",
       " 'spreads': 5,\n",
       " 'people': 6,\n",
       " 'underlying': 7,\n",
       " 'medical': 8,\n",
       " 'problems': 9,\n",
       " 'like': 10,\n",
       " 'cardiovascular': 11,\n",
       " 'elbow': 12,\n",
       " 'diabetes': 13,\n",
       " 'treatment': 14,\n",
       " 'chronic': 15,\n",
       " 'cancer': 16,\n",
       " 'likely': 17,\n",
       " 'develop': 18,\n",
       " 'serious': 19,\n",
       " 'older': 20,\n",
       " 'without': 21,\n",
       " 'special': 22,\n",
       " 'requiring': 23,\n",
       " 'best': 24,\n",
       " 'recover': 25,\n",
       " 'illness': 26,\n",
       " 'moderate': 27,\n",
       " 'mild': 28,\n",
       " 'experience': 29,\n",
       " 'coronavirusmost': 30,\n",
       " 'discovered': 31,\n",
       " 'newly': 32,\n",
       " 'caused': 33,\n",
       " 'infectious': 34,\n",
       " 'illnessthe': 35,\n",
       " 'way': 36,\n",
       " 'flexed': 37,\n",
       " 'coughs': 38,\n",
       " 'primarily': 39,\n",
       " 'droplets': 40,\n",
       " 'saliva': 41,\n",
       " 'discharge': 42,\n",
       " 'nose': 43,\n",
       " 'person': 44,\n",
       " 'sneezes': 45,\n",
       " 'prevent': 46,\n",
       " 'important': 47,\n",
       " 'also': 48,\n",
       " 'practice': 49,\n",
       " 'etiquette': 50,\n",
       " 'example': 51,\n",
       " 'coughing': 52,\n",
       " 'face': 53,\n",
       " 'touching': 54,\n",
       " 'frequently': 55,\n",
       " 'rub': 56,\n",
       " 'based': 57,\n",
       " 'alcohol': 58,\n",
       " 'using': 59,\n",
       " 'hands': 60,\n",
       " 'washing': 61,\n",
       " 'infection': 62,\n",
       " 'others': 63,\n",
       " 'protect': 64,\n",
       " 'causes': 65,\n",
       " 'informed': 66,\n",
       " 'well': 67,\n",
       " 'transmission': 68,\n",
       " 'slow': 69,\n",
       " 'coronavirus': 70}"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "1c2b733d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method KeyedVectors.get_vecattr of <gensim.models.keyedvectors.KeyedVectors object at 0x0000023C0D57D880>>"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "b06de421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8.6176936e-03  3.6709218e-03  5.1901368e-03  5.7368414e-03\n",
      "  7.4733677e-03 -6.1913831e-03  1.1114787e-03  6.0831690e-03\n",
      " -2.8408985e-03 -6.1907722e-03 -4.1391022e-04 -8.3867675e-03\n",
      " -5.5924854e-03  7.1185417e-03  3.3695656e-03  7.2230510e-03\n",
      "  6.8130400e-03  7.5293891e-03 -3.7978704e-03 -5.6777470e-04\n",
      "  2.3573311e-03 -4.5189606e-03  8.3933892e-03 -9.8801423e-03\n",
      "  6.7462446e-03  2.9046885e-03 -4.9544140e-03  4.3921475e-03\n",
      " -1.7397100e-03  6.7100707e-03  9.9702403e-03 -4.3752035e-03\n",
      " -5.8418897e-04 -5.7172272e-03  3.8447902e-03  2.8063066e-03\n",
      "  6.8961494e-03  6.1015161e-03  9.5198704e-03  9.2700776e-03\n",
      "  7.8990636e-03 -6.9817486e-03 -9.1460636e-03 -3.6333376e-04\n",
      " -3.0902263e-03  7.8927092e-03  5.9178984e-03 -1.5510609e-03\n",
      "  1.5132880e-03  1.7901127e-03  7.8273965e-03 -9.5252460e-03\n",
      " -2.0781127e-04  3.4574789e-03 -9.3968492e-04  8.3830301e-03\n",
      "  9.0228515e-03  6.5341969e-03 -7.0696609e-04  7.7171177e-03\n",
      " -8.5387509e-03  3.1963552e-03 -4.6369815e-03 -5.0778617e-03\n",
      "  3.5864965e-03  5.3877132e-03  7.7660247e-03 -5.7601631e-03\n",
      "  7.4109058e-03  6.6377046e-03 -3.7082615e-03 -8.7404530e-03\n",
      "  5.4538944e-03  6.5050209e-03 -7.5347838e-04 -6.7273127e-03\n",
      " -7.0692911e-03 -2.5029173e-03  5.1336112e-03 -3.6646959e-03\n",
      " -9.3932794e-03  3.8366453e-03  4.8706131e-03 -6.4008534e-03\n",
      "  1.1913148e-03 -2.0687059e-03  1.9956882e-05 -9.8792873e-03\n",
      "  2.7053759e-03 -4.7367825e-03  1.1167432e-03 -1.5780567e-03\n",
      "  2.2061900e-03 -7.8946073e-03 -2.7153553e-03  2.6718632e-03\n",
      "  5.3353477e-03 -2.3848745e-03 -9.5104696e-03  4.5050629e-03]\n"
     ]
    }
   ],
   "source": [
    "#Test the word Vector\n",
    "\n",
    "vector=model.wv['covid']\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "3c7e4b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is how it vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "812b3cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nose', 0.2063399851322174),\n",
       " ('sneezes', 0.17618191242218018),\n",
       " ('others', 0.15973719954490662),\n",
       " ('touching', 0.1527964174747467),\n",
       " ('develop', 0.14591798186302185)]"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#most similar words\n",
    "similar=model.wv.most_similar('infectious',topn=5)\n",
    "similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "c4aa086d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09081279"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#some words similarity\n",
    "model.wv.similarity(w1='infection',w2='develop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "f60cea42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'covid'"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filter on non similarity\n",
    "model.wv.doesnt_match([\"covid\",\"spread\",\"china\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "e2ac50dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#covid doesn't match with spread and china. spread and china are more similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aebe05",
   "metadata": {},
   "source": [
    "# Corse sine similarity (not needed to know just understand how we use)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf82026",
   "metadata": {},
   "source": [
    "from sklearn.manifold import TSNE\n",
    "vocab=['nose','sneezes','others','touching','develop','covid']\n",
    "def tsne_plot(model):\n",
    "    label=[]\n",
    "    wordvecs=[]\n",
    "    \n",
    "    for word in vocab:\n",
    "        wordvecs.append(model[word])\n",
    "        labels.append(word)\n",
    "        \n",
    "    tsne_model=TSNE(perplexity=3,n_components=2,init='pca',random_state=42)\n",
    "    coordinates=tsne_model.fit_transform(wordvecs)\n",
    "    \n",
    "    x=[]\n",
    "    y=[]\n",
    "    for value in coordinates:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(10,6))\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],xy=(x[i],y[i]),xytext=(2,2),textcoords='offset points',ha='right')\n",
    "        plt.show()\n",
    "        \n",
    "tsne_plot(model)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
